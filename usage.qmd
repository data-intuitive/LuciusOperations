---
title: Usage
engine: knitr
---

## Data processing and preparation

### Introduction

If the LuciusWeb interface is able to calculate Zhang scores and top tables sufficiently fast, it is because the effective calculations are performed in a distributed way by Spark running on a cluster. In order for Spark to do it's parallel magic, though, it needs the data to be in a suitable format. Every lookup or join that would have to be done during execution of a query would be detremental to the overall performance. In other words, for Spark to function effectively we need to prepare the data in a format that would be called _denormalized_ in traditional database terms.

What this means is that the database we work with on the level of the cluster is a (very) long list of complete records. If a specific compound appears 100 times in this database, than we store all that needs to be shown in the tables directly with _every_ record where this compound is the treatment. This approach trades in storage efficiency for processing speed.

We distinguish 3 main steps in preparing the data:

```{mermaid}
flowchart LR
  A[raw data] -- preprocessing --> B[preprocessed data per batch]
  B -- LuciusProcessing --> C[ready for Lucius]
```

The preprocessing step takes in the raw data from the experiments and applies differential analysis, possibly replicate consolidation and other computational steps. We will not discuss the _preprocessing_ step as it is outside the scope of our work. The result of this step is written to one or more data files as CSV, TSV, Parquet or some other structured data format. Typically this preprocessed data is structured by batch of the original raw data. Additional information should be provided with it dealing with annotations for treatments, genes and samples.

We pick up the data at this stage and prepare it for use with Lucius. That's what LuciusProcessing is for.

## `LuciusProcessing`

### Setup

We first have to initialize a spark-jobserver (and consequently Spark) context in which our subsequent jobs will run. We assume the `bin/build.sh` has been run already, that leaves us with:

```sh
utils/processing/create_context
```

You should receive a response containing `SUCCESS`. If you get a timeout message, please try again.

We should provide the appropriate JAR file to the jobserver as well:

```sh
utils/processing/upload_jar \
  --jars ... \           # location of the jar file
  --tag ...  \           # version of LuciusAPI to use
  --...
```

Please check if those arguments are not already set correctly in the `_viash.yaml` project config file.

:::{.callout-note}
If you decide to update the defaults in `_viash.yaml`, make sure to run `bin/build.sh` again!
:::

After you issued the previous command, you should receive a response saying the JAR is uploaded.

If you get something like this it means something is wrong with the JAR file:

```json
{
  "status": "ERROR",
  "result": "Binary is not of the right format"
}
```

### Introduction

LuciusProcessing transforms the data from preprocessed (per batch) data that is normalized to denormalized data in one or multiple 'files' per version (see later).

:::{.callout-note}
In what follows, we assume the preprocessed data is in Parquet format as well.
:::

The source should look like this:

```sh
input/<batch>_profile.parquet           # profile data (t-stats, p values, ...)
input/<batch>_profile_meta.parquet      # profile meta data (treatment, sample data, ...)
geneAnnotations                         # a 'file' containing gene annotations
treatmentAnnotations                    # a 'file' containing treatment annotations
cellAnnotations                         # a 'file' containing cell annotations
```
Typically, those files and directories will be on a shared filesystem or blob storage: S3 or HDFS.

Each `<batch>` of data might be added on a different date because experimental data is added. The preprocessed data we consume with LuciusProcessing has two modes:

1. The default mode, where _all_ data from `input/...` is fetched and processed. This results in a new major version.
2. An _incremental_ mode, used to process _new_ data since the last processing run.

### Full processing

Let's take a look at the workflow to better understand what happens. We initialized a context and uploaded a JAR above, it's now time to see if our config is right and if the data is available:

```
utils/processing/check
```

Here, we assume the default arguments are correctly configured in `_viash.yaml`, if in doubt you can always run `utils/processing/check -h`.

:::{.callout-remark}
We configured the `check` request to be synchronous. That means that the `check` tool will wait for the answer to be returned. There is however a (configurable) timeout for synchronous requests, and so it may be the returned status is `status: ERROR`. Don't worry in that case, there are ways to get to the requested information. We'll discuss them later.
:::

This is the output of `check` on a limited dataset, trimmed and formatted to be better readable:

```json
{
  "duration": "12.053 secs",
  "classPath": "com.dataintuitive.luciusprocessing.check",
  "startTime": "2022-11-30T13:14:01.863+01:00",
  "context": "luciusapi",
  "result": {
    "info": "No data to process, please check input and parameters",
    "header": "None",
    "data": {
      "inputPath": ".../",
      "inputs": [
        "batch1: 2022-10-25 with 162 samples stored in .../batch1_profile_meta.parquet",
        "batch2: 2022-10-25 with 84 samples stored in .../batch2_profile_meta/ASG001_MCF7_6H_l5_profile_meta.parquet",
        ...
        ],
      "filter_inputs": [
        ...
      ],
      "outputs": [
        "Version 6_0 at 2022-11-28 (.../output-data/2022-11-28_output_v6_0.parquet)",
        "Version 1_0 at 2022-12-01 (.../output-data/2022-12-01_output_v1_0.parquet)",
        ...
        ]
    }
  },
  "status": "FINISHED",
  "jobId": "c6ea3c8a-dcd1-41bb-bf28-69c46e5431f7",
  "contextId": ""
}
```

There are 3 important lists in the output above:

1. `inputs` is a list of the preprocessed batches that are available.
2. `filter_inputs` is for running incrementally, we'll discuss that later.
3. `outputs` is a list of already processed files.

If you add `--processingDate` to the `check` tool, only data _before_ that date will be processed. By default it is the current date of processing, but in certain situations it might be useful to be able to set it explicitly. For instance, if you want to process the a large number of batches in pieces based on the date at which they were added. Or just when preparing a test dataset. But in general, it should not be used.

If you specify the `--processingDate`, the `filter_inputs` list will contain the entries that _match_ the query.

When the output of the `check` tool yields the expected result, it's time to start processinsg the data. If processed data is available in the `outputs` list, the major version will automatically be updated when a full processing run is started. For example, if the latest output dataset is version 3.x, the next full processing run will get version 4.

The dates and versions are only encoded in the filenames of the Parquet files for convenience. The dates and version that are effectively used are encoded inside thte files. So even if we rename files or move them, we are able to retrieve that information.

It's now time to start the effective processing job:

```sh
utils/processing/process
```

Again, this just works if the `_viash.yaml` config file has been properly provisioned or configured.

The `process` tool does not wait for the result and runs in the background. One can either connect to the jobserver console or via the CLI (see later).

### Incremental processing runs

The process for incremental runs is the same as for full runs, the difference is in the selection of data that is used for processing: an incremental run looks at the last major version in the `output` location, and for that version the last minor version. It then checks at what date that data was processed. If there is input data that is _newer_ than the last processed data, it will be processed and the minor version will be bumped.

Running the processing is as easy as before by adding `--incremental`:

```sh
utils/processing/process --incremental
```

Remember that the `filtered_inputs` will contain the entries in the input that would be processed when running incrementally.

In order to cleanup, it's good practice to remove the context after we ran the processing:

```sh
utils/processing/remove_context
```

## `LuciusAPI`

Once there data is processed for use with Lucius, it's a matter of initializing the long-running context. This is similar to what we did above:

```sh
utils/api/create_context
utils/api/upload_jar
utils/api/initialize
```

At this point, the data will be loaded and cached in memory on the Spark cluster. The caching is important for performance. You can check the status of the initialization job using the jobserver console or the CLI (see below under Troubleshooting).

When the `initialization` job has finished, a check can be performed:

```sh
utils/api/check
```

You'll get some basic statistics about what is available in the dataset. If this works, there's a high chance that the frontend will work!

:::{.callout-warning}
Do not remove the context unless you really intend to do that: removing the context will stop the application from working.
:::

## Customizations


## Processed data versions

As noted above, processed data is stored with a major/minor versioning scheme: major versions for full processing runs, minor versions for incremental runs. Each new full run generates a new major version and consequently the `utils/api/initialization` tool should pick out the correct one to initialize the context.

The default is set to 0, but one will often require different versions. One approach is to update the `_viash.yaml` file and encode the proper default there.

Another approach is to simply uses the arguments to modify the behavior of the `initialize` tool:

```sh
utils/api/initialize --db_version 2
```

Please note that we only select a major version because all minor versions are incremental runs on top of version 2.0.

### Other customizations

It's possible to edit the `_viash.yaml` file and run `bin/build.sh` again in order to create the `utils/` toolbox with new defaults. It's also possible to use the arguments available, just be sure to use them consistenly.

For instance:

```sh
bin/processing/create_context \
  --application my_app_name       # specify a different name
bin/processing/upload_jar \
  --tag 0.2.0                     # select an older version
bin/processing/check \
  --application my_app_name       # use the same app name as before
bin/api/initialize \
  --db $different_path \          # point to different database location
  --db_version 1  \               # point to different database version
  --application my_app_name
```

As always, simply call the appropriate tool from the toolbox with `-h` or `--help`.

In principle, we could create scripts (or even workflows?) based on these steps.

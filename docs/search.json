[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "LuciusOperations",
    "section": "",
    "text": "The glue that holds the Compass stack together\n\n\n\nThis repo is a toolbox for the maintenance of the Lucius stack. The main goal is to make life easier initializing/running/checking the APIs that are part of the stack: LuciusAPI and LuciusProcessing. We might add functionality for Sourire (and perhaps even Brutus) later as well.\nLuciusAPI and LuciusProcessing are two REST APIs that are both endpoints defined using the Spark-Jobserver project and that expose complementary functionality:\n\n\nLuciusProcessing deals with preparing source data into a format that can be initialized for LuciusAPI.\nLuciusProcessing only exposes 2 endpoints: a process endpoint for processing the data from input to LuciusAPI-ready output and a check endpoint for retrieving information about source and (pre-computed) output.\n\n\n\nLuciusAPI is the engine for the LuciusWeb web frontend. It defines endpoints for all the queries and computations that need to be performed in the application. A Spark context is running 24x7 and LuciusAPI acts as the interface to that Spark context.\nLuciusAPI contains many more endpoints for every kind of query of computation that needs to be available to the frontend. The concrete implementation of these endpoints is provided by the [LuciusCore] library: Zhang correlation calculation, histogram, …\n\n\n\n\nConnecting to those APIs is not hard: in principle one sends a configuration object to the API and one gets a JSON object back. It does requires a very strict format for input and parameters and is cumbersome to write though.\nIn order to aid in connecting to the APIs and as such also perform basic processing and initialization tasks, we developed the LuciusOperations toolbox.\n\n\n\nThe short version is to fetch the LuciusOperations repository, make sure you have Java 8 or higher installed and run bin/build.sh. After this, the tools for LuciusProcessing and LuciusAPI are respectively under utils/processing and utils/api:\ngit clone https://github.com/data-intuitive/LuciusOperations\ncd LuciusOperations\nbin/build.sh\nThe following tools will be created:\nutils\n├── api\n│   ├── check\n│   ├── create_context\n│   ├── fetch_jar\n│   ├── initialize\n│   ├── remove_context\n│   └── upload_jar\n└── processing\n    ├── check\n    ├── create_context\n    ├── fetch_jar\n    ├── process\n    ├── remove_context\n    └── upload_jar\n    ```\n\nEvery tool in the toolbox is a (Viash) standalone script that contains it's own help, i.e.:\n\n❯ utils/processing/check -h check dev\nArguments: –endpoint type: string default: http://localhost:8090 The endpoint (URL) to connect to\n… ```\nPlease refer to the installation instructions for more information about what is happening behind the scenes.\n\n\n\nA typical workflow would look like this:\n\nEnsure the spark-jobserver is running\nMake sure the JAR files for LuciusAPI and LuciusProcessing are available in a local directory, preferably one next to the other. Those JAR files are typically built using a CI pipeline, but can be fetched from the github repository as well.\nUpdate the _viash.yaml configuration file for your environment.\nrun bin/build.sh again, as this will build the different tools with the correct defaults as provided in the _viash.yaml config file.\nRun the following commands:\nutils/processing/upload_jar        # Upload both JAR files\nutils/api/upload_jar               \nutils/processing/create_context    # create the context\nutils/processing/process           # start the processing job\nutils/processing/check             # verify if output is written\nutils/api/initialize               # initialize Spark job\nutils/api/check                    # after some time, check\n\n\n\n\nThe following are required in order to run the components from the toolbox:\n\nwget\nbash\ncurl\n\nOne day, we might allow HTTPie to be used for a more user friendly way of calling the endpoints."
  },
  {
    "objectID": "README.html#introduction",
    "href": "README.html#introduction",
    "title": "LuciusOperations",
    "section": "",
    "text": "This repo is a toolbox for the maintenance of the Lucius stack. The main goal is to make life easier initializing/running/checking the APIs that are part of the stack: LuciusAPI and LuciusProcessing. We might add functionality for Sourire (and perhaps even Brutus) later as well.\nLuciusAPI and LuciusProcessing are two REST APIs that are both endpoints defined using the Spark-Jobserver project and that expose complementary functionality:\n\n\nLuciusProcessing deals with preparing source data into a format that can be initialized for LuciusAPI.\nLuciusProcessing only exposes 2 endpoints: a process endpoint for processing the data from input to LuciusAPI-ready output and a check endpoint for retrieving information about source and (pre-computed) output.\n\n\n\nLuciusAPI is the engine for the LuciusWeb web frontend. It defines endpoints for all the queries and computations that need to be performed in the application. A Spark context is running 24x7 and LuciusAPI acts as the interface to that Spark context.\nLuciusAPI contains many more endpoints for every kind of query of computation that needs to be available to the frontend. The concrete implementation of these endpoints is provided by the [LuciusCore] library: Zhang correlation calculation, histogram, …"
  },
  {
    "objectID": "README.html#cli-management-using-luciusoperations",
    "href": "README.html#cli-management-using-luciusoperations",
    "title": "LuciusOperations",
    "section": "",
    "text": "Connecting to those APIs is not hard: in principle one sends a configuration object to the API and one gets a JSON object back. It does requires a very strict format for input and parameters and is cumbersome to write though.\nIn order to aid in connecting to the APIs and as such also perform basic processing and initialization tasks, we developed the LuciusOperations toolbox."
  },
  {
    "objectID": "README.html#installation",
    "href": "README.html#installation",
    "title": "LuciusOperations",
    "section": "",
    "text": "The short version is to fetch the LuciusOperations repository, make sure you have Java 8 or higher installed and run bin/build.sh. After this, the tools for LuciusProcessing and LuciusAPI are respectively under utils/processing and utils/api:\ngit clone https://github.com/data-intuitive/LuciusOperations\ncd LuciusOperations\nbin/build.sh\nThe following tools will be created:\nutils\n├── api\n│   ├── check\n│   ├── create_context\n│   ├── fetch_jar\n│   ├── initialize\n│   ├── remove_context\n│   └── upload_jar\n└── processing\n    ├── check\n    ├── create_context\n    ├── fetch_jar\n    ├── process\n    ├── remove_context\n    └── upload_jar\n    ```\n\nEvery tool in the toolbox is a (Viash) standalone script that contains it's own help, i.e.:\n\n❯ utils/processing/check -h check dev\nArguments: –endpoint type: string default: http://localhost:8090 The endpoint (URL) to connect to\n… ```\nPlease refer to the installation instructions for more information about what is happening behind the scenes."
  },
  {
    "objectID": "README.html#usage",
    "href": "README.html#usage",
    "title": "LuciusOperations",
    "section": "",
    "text": "A typical workflow would look like this:\n\nEnsure the spark-jobserver is running\nMake sure the JAR files for LuciusAPI and LuciusProcessing are available in a local directory, preferably one next to the other. Those JAR files are typically built using a CI pipeline, but can be fetched from the github repository as well.\nUpdate the _viash.yaml configuration file for your environment.\nrun bin/build.sh again, as this will build the different tools with the correct defaults as provided in the _viash.yaml config file.\nRun the following commands:\nutils/processing/upload_jar        # Upload both JAR files\nutils/api/upload_jar               \nutils/processing/create_context    # create the context\nutils/processing/process           # start the processing job\nutils/processing/check             # verify if output is written\nutils/api/initialize               # initialize Spark job\nutils/api/check                    # after some time, check"
  },
  {
    "objectID": "README.html#prerequisites",
    "href": "README.html#prerequisites",
    "title": "LuciusOperations",
    "section": "",
    "text": "The following are required in order to run the components from the toolbox:\n\nwget\nbash\ncurl\n\nOne day, we might allow HTTPie to be used for a more user friendly way of calling the endpoints."
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Usage",
    "section": "",
    "text": "If the LuciusWeb interface is able to calculate Zhang scores and top tables sufficiently fast, it is because the effective calculations are performed in a distributed way by Spark running on a cluster. In order for Spark to do it’s parallel magic, though, it needs the data to be in a suitable format. Every lookup or join that would have to be done during execution of a query would be detremental to the overall performance. In other words, for Spark to function effectively we need to prepare the data in a format that would be called denormalized in traditional database terms.\nWhat this means is that the database we work with on the level of the cluster is a (very) long list of complete records. If a specific compound appears 100 times in this database, than we store all that needs to be shown in the tables directly with every record where this compound is the treatment. This approach trades in storage efficiency for processing speed.\nWe distinguish 3 main steps in preparing the data:\n\n\n\n\nflowchart LR\n  A[raw data] -- preprocessing --&gt; B[preprocessed data per batch]\n  B -- LuciusProcessing --&gt; C[ready for Lucius]\n\n\n\n\n\nThe preprocessing step takes in the raw data from the experiments and applies differential analysis, possibly replicate consolidation and other computational steps. We will not discuss the preprocessing step as it is outside the scope of our work. The result of this step is written to one or more data files as CSV, TSV, Parquet or some other structured data format. Typically this preprocessed data is structured by batch of the original raw data. Additional information should be provided with it dealing with annotations for treatments, genes and samples.\nWe pick up the data at this stage and prepare it for use with Lucius. That’s what LuciusProcessing is for."
  },
  {
    "objectID": "usage.html#data-processing-and-preparation",
    "href": "usage.html#data-processing-and-preparation",
    "title": "Usage",
    "section": "",
    "text": "If the LuciusWeb interface is able to calculate Zhang scores and top tables sufficiently fast, it is because the effective calculations are performed in a distributed way by Spark running on a cluster. In order for Spark to do it’s parallel magic, though, it needs the data to be in a suitable format. Every lookup or join that would have to be done during execution of a query would be detremental to the overall performance. In other words, for Spark to function effectively we need to prepare the data in a format that would be called denormalized in traditional database terms.\nWhat this means is that the database we work with on the level of the cluster is a (very) long list of complete records. If a specific compound appears 100 times in this database, than we store all that needs to be shown in the tables directly with every record where this compound is the treatment. This approach trades in storage efficiency for processing speed.\nWe distinguish 3 main steps in preparing the data:\n\n\n\n\nflowchart LR\n  A[raw data] -- preprocessing --&gt; B[preprocessed data per batch]\n  B -- LuciusProcessing --&gt; C[ready for Lucius]\n\n\n\n\n\nThe preprocessing step takes in the raw data from the experiments and applies differential analysis, possibly replicate consolidation and other computational steps. We will not discuss the preprocessing step as it is outside the scope of our work. The result of this step is written to one or more data files as CSV, TSV, Parquet or some other structured data format. Typically this preprocessed data is structured by batch of the original raw data. Additional information should be provided with it dealing with annotations for treatments, genes and samples.\nWe pick up the data at this stage and prepare it for use with Lucius. That’s what LuciusProcessing is for."
  },
  {
    "objectID": "usage.html#luciusprocessing",
    "href": "usage.html#luciusprocessing",
    "title": "Usage",
    "section": "LuciusProcessing",
    "text": "LuciusProcessing\n\nGeneric setup information\nWe first have to initialize a spark-jobserver (and consequently Spark) context in which our subsequent jobs will run. We assume the bin/build.sh has been run already, that leaves us with:\nutils/processing/create_context\nYou should receive a response containing SUCCESS. If you get a timeout message, please try again.\nWe should provide the appropriate JAR file to the jobserver as well:\nutils/processing/upload_jar \\\n  --jars ... \\           # location of the jar file\n  --tag ...  \\           # version of LuciusAPI to use\n  --...\nPlease check if those arguments are not already set correctly in the _viash.yaml project config file.\n\n\n\n\n\n\n\n\n\n\nIf you decide to update the defaults in _viash.yaml, make sure to run bin/build.sh again!\n\n\nAfter you issued the previous command, you should receive a response saying the JAR is uploaded.\nIf you get something like this it means something is wrong with the JAR file:\n{\n  \"status\": \"ERROR\",\n  \"result\": \"Binary is not of the right format\"\n}\n\n\nSuggested setup\nWhen we deploy LuciusAPI and LuciusOperations, we build/deploy the JAR files on one instance (or pod) together with the Spark Jobserver runtime. Next, we LuciusOperations to initialize the LuciusAPI backend on the instance itself. This means that all necessary ingredients for running LuciusOperations (both the api and the processing) part are available on the host:\n\nThe necessary JAR files\nThe Spark Jobserver endpoint (it’s just localhost:8090)\nA working _viash.yaml configuration file\n\n\n\n\n\n\n\n\n\n\n\nThe login info for this instance is project-specific and will be provided separately.\n\n\nOpen a shell on the jobserver instance, let’s assume the following directory structure:\n/home\n  LuciusAPI\n  LuciusProcessing\n  luciusoperations\n  LuciusAPI.jar\n  LuciusProcessing.jar\nSince this is a running Spark-Jobserver instance (contrary to the generic setup above), the configuration under luciusoperations is guaranteed to be correct1.\n\nIn order to avoid interferring with the running services and applications, we make a copy of the luciusoperations directory first:\ncd /home\ncp -r luciusoperations luciusoperations-process\ncd luciusoperations-process\nNow, open the _viash.yaml file and replace REPLACE_ME with luciusprocessing. Alternatively, use the following sed instruction to achieve the same from the CLI directly:\nsed -i 's/REPLACE_ME/luciusprocessing/' _viash.yaml\nRun bin/build.sh in order to update the tools under utils/.\nInitialize a new context:\nutils/processing/create_context\nThis can take minute to complete, the result should be a status message in JSON format saying Context initialized.\nUpload the JAR file:\nutils/processing/upload_jar\nSince the JAR is uploaded to localhost (on port 8090 to be precise), this is fast.\nRun the processing job. See further down this document for more information about incremental and full processing jobs and how to configure those. For completeness, if a full processing job is required this would entail running:\nutils/processing/process\n\nThe process may take a lot of time, and that’s expected. It can be monitored by opening the Spark Jobserver console.\n\n\nWhat is processed?\nLuciusProcessing transforms the data from preprocessed (per batch) data that is normalized to denormalized data in one or multiple ‘files’ per version (see later).\n\n\n\n\n\n\n\n\n\n\nIn what follows, we assume the preprocessed data is in Parquet format as well.\n\n\nThe source should look like this:\ninput/&lt;batch&gt;_profile.parquet           # profile data (t-stats, p values, ...)\ninput/&lt;batch&gt;_profile_meta.parquet      # profile meta data (treatment, sample data, ...)\ngeneAnnotations                         # a 'file' containing gene annotations\ntreatmentAnnotations                    # a 'file' containing treatment annotations\ncellAnnotations                         # a 'file' containing cell annotations\nTypically, those files and directories will be on a shared filesystem or blob storage: S3 or HDFS.\nEach &lt;batch&gt; of data might be added on a different date because experimental data is added. The preprocessed data we consume with LuciusProcessing has two modes:\n\nThe default mode, where all data from input/... is fetched and processed. This results in a new major version.\nAn incremental mode, used to process new data since the last processing run.\n\n\n\nFull processing\nLet’s take a look at the workflow to better understand what happens. We initialized a context and uploaded a JAR above, it’s now time to see if our config is right and if the data is available:\nutils/processing/check\nHere, we assume the default arguments are correctly configured in _viash.yaml, if in doubt you can always run utils/processing/check -h.\n\nWe configured the check request to be synchronous. That means that the check tool will wait for the answer to be returned. There is however a (configurable) timeout for synchronous requests, and so it may be the returned status is status: ERROR. Don’t worry in that case, there are ways to get to the requested information. We’ll discuss them later.\n\nThis is the output of check on a limited dataset, trimmed and formatted to be better readable:\n{\n  \"duration\": \"12.053 secs\",\n  \"classPath\": \"com.dataintuitive.luciusprocessing.check\",\n  \"startTime\": \"2022-11-30T13:14:01.863+01:00\",\n  \"context\": \"luciusapi\",\n  \"result\": {\n    \"info\": \"No data to process, please check input and parameters\",\n    \"header\": \"None\",\n    \"data\": {\n      \"inputPath\": \".../\",\n      \"inputs\": [\n        \"batch1: 2022-10-25 with 162 samples stored in .../batch1_profile_meta.parquet\",\n        \"batch2: 2022-10-25 with 84 samples stored in .../batch2_profile_meta/ASG001_MCF7_6H_l5_profile_meta.parquet\",\n        ...\n        ],\n      \"filter_inputs\": [\n        ...\n      ],\n      \"outputs\": [\n        \"Version 6_0 at 2022-11-28 (.../output-data/2022-11-28_output_v6_0.parquet)\",\n        \"Version 1_0 at 2022-12-01 (.../output-data/2022-12-01_output_v1_0.parquet)\",\n        ...\n        ]\n    }\n  },\n  \"status\": \"FINISHED\",\n  \"jobId\": \"c6ea3c8a-dcd1-41bb-bf28-69c46e5431f7\",\n  \"contextId\": \"\"\n}\nThere are 3 important lists in the output above:\n\ninputs is a list of the preprocessed batches that are available.\nfilter_inputs is for running incrementally, we’ll discuss that later.\noutputs is a list of already processed files.\n\nIf you add --processingDate to the check tool, only data before that date will be processed. By default it is the current date of processing, but in certain situations it might be useful to be able to set it explicitly. For instance, if you want to process the a large number of batches in pieces based on the date at which they were added. Or just when preparing a test dataset. But in general, it should not be used.\nIf you specify the --processingDate, the filter_inputs list will contain the entries that match the query.\nWhen the output of the check tool yields the expected result, it’s time to start processinsg the data. If processed data is available in the outputs list, the major version will automatically be updated when a full processing run is started. For example, if the latest output dataset is version 3.x, the next full processing run will get version 4.\nThe dates and versions are only encoded in the filenames of the Parquet files for convenience. The dates and version that are effectively used are encoded inside thte files. So even if we rename files or move them, we are able to retrieve that information.\nIt’s now time to start the effective processing job:\nutils/processing/process\nAgain, this just works if the _viash.yaml config file has been properly provisioned or configured.\nThe process tool does not wait for the result and runs in the background. One can either connect to the jobserver console or via the CLI (see later).\n\n\nIncremental processing runs\nThe process for incremental runs is the same as for full runs, the difference is in the selection of data that is used for processing: an incremental run looks at the last major version in the output location, and for that version the last minor version. It then checks at what date that data was processed. If there is input data that is newer than the last processed data, it will be processed and the minor version will be bumped.\nRunning the processing is as easy as before by adding --incremental:\nutils/processing/process --incremental\nRemember that the filtered_inputs will contain the entries in the input that would be processed when running incrementally.\nIn order to cleanup, it’s good practice to remove the context after we ran the processing:\nutils/processing/remove_context"
  },
  {
    "objectID": "usage.html#luciusapi",
    "href": "usage.html#luciusapi",
    "title": "Usage",
    "section": "LuciusAPI",
    "text": "LuciusAPI\nOnce there data is processed for use with Lucius, it’s a matter of initializing the long-running context. This is similar to what we did above:\nutils/api/create_context\nutils/api/upload_jar\nutils/api/initialize\nAt this point, the data will be loaded and cached in memory on the Spark cluster. The caching is important for performance. You can check the status of the initialization job using the jobserver console or the CLI (see below under Troubleshooting).\nPlease note that while the processing step only requires the profiles, profile meta and gene annotations, LuciusAPI also needs treatment annotations and cell annotations. Those have to be provided.\nWhen the initialization job has finished, a check can be performed:\nutils/api/check\nYou’ll get some basic statistics about what is available in the dataset. If this works, there’s a high chance that the frontend will work!\n\n\n\n\n\n\n\n\n\n\nDo not remove the context unless you really intend to do that: removing the context will stop the application from working.\n\n\n\nTechnical details\nEvery tool in the toolbox in fact performs a POST REST request to LuciusAPI or LuciusProcessing. This POST request includes a so-called payload or a configuration object for this specific endpoint or function. The processing/process and api/initialize also require such a configuration payload. The template for this can be found under etc/. There is one config file for LuciusAPI and one for LuciusProcessing. A simple templating mechanism is used where variables that require subsitition are enclosed by two underscores (__)."
  },
  {
    "objectID": "usage.html#customizations",
    "href": "usage.html#customizations",
    "title": "Usage",
    "section": "Customizations",
    "text": "Customizations\n\nProcessed data versions\nAs noted above, processed data is stored with a major/minor versioning scheme: major versions for full processing runs, minor versions for incremental runs. Each new full run generates a new major version and consequently the utils/api/initialization tool should pick out the correct one to initialize the context.\nThe default is set to 0, but one will often require different versions. One approach is to update the _viash.yaml file and encode the proper default there.\nAnother approach is to simply uses the arguments to modify the behavior of the initialize tool:\nutils/api/initialize --db_version 2\nPlease note that we only select a major version because all minor versions are incremental runs on top of version 2.0.\n\n\nOther customizations\nIt’s possible to edit the _viash.yaml file and run bin/build.sh again in order to create the utils/ toolbox with new defaults. It’s also possible to use the arguments available, just be sure to use them consistenly.\nFor instance:\nbin/processing/create_context \\\n  --application my_app_name       # specify a different name\nbin/processing/upload_jar \\\n  --tag 0.2.0                     # select an older version\nbin/processing/check \\\n  --application my_app_name       # use the same app name as before\nbin/api/initialize \\\n  --db $different_path \\          # point to different database location\n  --db_version 1  \\               # point to different database version\n  --application my_app_name\nAs always, simply call the appropriate tool from the toolbox with -h or --help.\nIn principle, we could create scripts (or even workflows?) based on these steps."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation Instructions",
    "section": "",
    "text": "LuciusOperations uses Viash to convert relatively simple scripts containing REST calls into configurable CLI tools.\nThe use of Viash and other applications of the tool is outside the scope of this document. Suffice to note that Viash uses a concept of source and what is built from that source. The source of the components can be found under src/, using the bin/build.sh wrapper script, the built scripts are under utils/.\n\nbin/build.sh\n\n\n&gt;&gt; Building both namespaces\nExporting create_context (processing) =native=&gt; utils/processing\nExporting upload_jar (processing) =native=&gt; utils/processing\nExporting remove_context (processing) =native=&gt; utils/processing\nExporting fetch_jar (processing) =native=&gt; utils/processing\nExporting check (processing) =native=&gt; utils/processing\nExporting process (processing) =native=&gt; utils/processing\nNot all configs built successfully\n  7/13 configs were disabled\n  6/13 configs built successfully\nExporting create_context (api) =native=&gt; utils/api\nExporting upload_jar (api) =native=&gt; utils/api\nExporting initialize (api) =native=&gt; utils/api\nExporting remove_context (api) =native=&gt; utils/api\nExporting fetch_jar (api) =native=&gt; utils/api\nExporting check (api) =native=&gt; utils/api\nNot all configs built successfully\n  7/13 configs were disabled\n  6/13 configs built successfully\nExporting workflow (load) =native=&gt; utils/load\nNot all configs built successfully\n  12/13 configs were disabled\n  1/13 configs built successfully\n\n&gt;&gt; Please see under utils/ to find the tools for api and processing...\n\n\n\n\n\n\n\n\n\n\n\n\nThis description deals with the installation and configuration of LuciusOperations only."
  },
  {
    "objectID": "installation.html#introduction",
    "href": "installation.html#introduction",
    "title": "Installation Instructions",
    "section": "",
    "text": "LuciusOperations uses Viash to convert relatively simple scripts containing REST calls into configurable CLI tools.\nThe use of Viash and other applications of the tool is outside the scope of this document. Suffice to note that Viash uses a concept of source and what is built from that source. The source of the components can be found under src/, using the bin/build.sh wrapper script, the built scripts are under utils/.\n\nbin/build.sh\n\n\n&gt;&gt; Building both namespaces\nExporting create_context (processing) =native=&gt; utils/processing\nExporting upload_jar (processing) =native=&gt; utils/processing\nExporting remove_context (processing) =native=&gt; utils/processing\nExporting fetch_jar (processing) =native=&gt; utils/processing\nExporting check (processing) =native=&gt; utils/processing\nExporting process (processing) =native=&gt; utils/processing\nNot all configs built successfully\n  7/13 configs were disabled\n  6/13 configs built successfully\nExporting create_context (api) =native=&gt; utils/api\nExporting upload_jar (api) =native=&gt; utils/api\nExporting initialize (api) =native=&gt; utils/api\nExporting remove_context (api) =native=&gt; utils/api\nExporting fetch_jar (api) =native=&gt; utils/api\nExporting check (api) =native=&gt; utils/api\nNot all configs built successfully\n  7/13 configs were disabled\n  6/13 configs built successfully\nExporting workflow (load) =native=&gt; utils/load\nNot all configs built successfully\n  12/13 configs were disabled\n  1/13 configs built successfully\n\n&gt;&gt; Please see under utils/ to find the tools for api and processing...\n\n\n\n\n\n\n\n\n\n\n\n\nThis description deals with the installation and configuration of LuciusOperations only."
  },
  {
    "objectID": "installation.html#where-to-run",
    "href": "installation.html#where-to-run",
    "title": "Installation Instructions",
    "section": "Where to run",
    "text": "Where to run\n\nGeneric installation\nIt does not matter where the tools in this toolset are used, as long as wget and curl are available on the device and the DNS endpoints of the Spark Jobserver can be resolved, it should work.\nIn order to run the processing jobs in api and processing, however, it’s important to collect the appropriate JAR files. Those define the logic (i.e. the code) to actually run on the Spark cluster. Both the api as well as the processing toolset have an upload_jar tool that can be used to upload the appropriate JAR file to the Spark Jobserver.\nWe will often run the LuciusOperations tools from the SparkJobser instance itself. This allows us to use the (locally) available JAR files that have been used to initialize the API in the first place. Connecting to a Spark Jobserver instance depends on your installation.\n\n\nSuggested installation\nWe suggest to run the LuciusOperations tools from an instance running the Lucius backend. Nothing has to be installed in that case, please see here for more information."
  },
  {
    "objectID": "installation.html#technical-details",
    "href": "installation.html#technical-details",
    "title": "Installation Instructions",
    "section": "Technical details",
    "text": "Technical details\nEvery Viash component contains a script.sh file (usually some parameter handling and a command to execute) and a config.vsh.yaml file which contains the configuration for the component.\nBy running bin/build.sh, this combination of files is transformed into 1 executable script that performs essentially two things:\n\nThe resulting executable contains the argument parsing necessary to run it from the CLI\nThe defaults configured in _viash.yaml are applied.\n\nAn illustration of the CLI argument parsing capabilities:\n\nutils/processing/check -h | head\n\ncheck dev\n\nArguments:\n    --endpoint\n        type: string\n        default: http://localhost:8090\n        The endpoint (URL) to connect to\n\n    --application\n        type: string\n\n\nThe format of the configuration in _viash.yaml is derived from the way jq allows to query and update JSON blobs. It should be clear from the example in the repository how to use it:\nsource: src\ntarget: utils\n\nconfig_mods: |\n  .functionality.version := 'dev'\n  .functionality.arguments[.name == '--db'].default := '/Users/toni/Dropbox/_GSK/2022/output-data'\n  .functionality.arguments[.name == '--db_version'].default := 0\n  .functionality.arguments[.name == '--input'].default := '/Users/toni/Dropbox/_GSK/2022/l1k_l5_subset'\n  .functionality.arguments[.name == '--application'].default := 'luciusapi'\n  .functionality.arguments[.name == '--geneAnnotations'].default := '/Users/toni/Dropbox/_GSK/2022/l1k_l5_subset/l1k_gene_xref/l1k_gene_xref.parquet'\n  .functionality.arguments[.name == '--treatmentAnnotations'].default := '/Users/toni/Dropbox/_GSK/2022/l1k_l5_subset/pert_xref/pert_xref.parquet'\n  .functionality.arguments[.name == '--cellAnnotations'].default := '/Users/toni/Dropbox/_GSK/2022/l1k_l5_subset/cell_xref/cell_xref.parquet'\nThe bin/build.sh script uses Viash to create 2 subdirectories under utils/ containing tools that are used for the processing part and for the api part:\n\ntree utils/\n\nutils/\n├── api\n│   ├── check\n│   ├── create_context\n│   ├── fetch_jar\n│   ├── initialize\n│   ├── remove_context\n│   └── upload_jar\n├── load\n│   └── workflow\n├── native\n│   └── load\n│       └── workflow\n│           └── workflow\n└── processing\n    ├── check\n    ├── create_context\n    ├── fetch_jar\n    ├── process\n    ├── remove_context\n    └── upload_jar\n\n7 directories, 14 files\n\n\nPlease refer to the usage guide for more information about how to use the tools under utils/."
  },
  {
    "objectID": "tips.html",
    "href": "tips.html",
    "title": "Tips",
    "section": "",
    "text": "Connect to the URL of the spark-jobserver instance in your environment. If you don’t know what that is, but you’re LuciusOperations toolbox has been configured correctly, you can retrieve that information from the output of utils/processing/check -h. The first argument in the help message is --endpoint and the default value for it is the URL you need:\nutils/processing/check -h | head -8\nOpen a browser tab and connect to this URL, you should see a screen similar to the one below.\n\n\n\nSpark Jobserver console\n\n\nThere are three tabs here:\n\nJobs: to retrieve a list of recently run jobs split in 3 ‘states’: Running, Completed and Failed.\nContexts: to retrieve a list of contexts that are services from this jobserver instance. This should correspond to the context created using utils/processing/create_context.\nBinaries: this tab lists the JAR files that have been uploaded using utils/processing/upload_jar.\n\nClicking on the the job link in the Jobs tab, you get the JSON result of the job. The (C) link retrieves the configuration that was used for this job.\nIf you know the job number, you could alternatively open the job page directly:\n&lt;endpoint&gt;/jobs/&lt;jobID&gt;"
  },
  {
    "objectID": "tips.html#spark-jobserver-console",
    "href": "tips.html#spark-jobserver-console",
    "title": "Tips",
    "section": "",
    "text": "Connect to the URL of the spark-jobserver instance in your environment. If you don’t know what that is, but you’re LuciusOperations toolbox has been configured correctly, you can retrieve that information from the output of utils/processing/check -h. The first argument in the help message is --endpoint and the default value for it is the URL you need:\nutils/processing/check -h | head -8\nOpen a browser tab and connect to this URL, you should see a screen similar to the one below.\n\n\n\nSpark Jobserver console\n\n\nThere are three tabs here:\n\nJobs: to retrieve a list of recently run jobs split in 3 ‘states’: Running, Completed and Failed.\nContexts: to retrieve a list of contexts that are services from this jobserver instance. This should correspond to the context created using utils/processing/create_context.\nBinaries: this tab lists the JAR files that have been uploaded using utils/processing/upload_jar.\n\nClicking on the the job link in the Jobs tab, you get the JSON result of the job. The (C) link retrieves the configuration that was used for this job.\nIf you know the job number, you could alternatively open the job page directly:\n&lt;endpoint&gt;/jobs/&lt;jobID&gt;"
  },
  {
    "objectID": "tips.html#running-remotely",
    "href": "tips.html#running-remotely",
    "title": "Tips",
    "section": "Running remotely",
    "text": "Running remotely\nIn many cases, we will run the LuciusOperations on the same instance that the spark jobserver itself is running. This means that the endpoint to connect to will often be:\nhttp://localhost:8090\nSince the spark jobserver is a REST service, you don’t have to be logged in via a remote session to this instance itself. If you know the URL of the spark jobserver, it’s easy to run the tools with that:\nutils/processing/check --endpoint https://...\nAlternatively, you can add the following line to the _viash.yaml file and run bin/build.sh again:\n.functionality.arguments[.name == '--endpoint'].default := 'https://...'\nThis is a very handy way to verify if the backend of Lucius is still running:\nutils/api/check --endpoint https://..."
  },
  {
    "objectID": "tips.html#getting-job-output-when-the-request-times-out",
    "href": "tips.html#getting-job-output-when-the-request-times-out",
    "title": "Tips",
    "section": "Getting job output when the request times out",
    "text": "Getting job output when the request times out\nSometimes, the following message or something similar can be returned:\n{\n  \"status\": \"ERROR\",\n  \"result\": {\n    \"message\": \"Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\",\n    \"errorClass\": \"akka.pattern.AskTimeoutException\",\n    \"stack\": \"akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\\n\\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)\\n\\tat akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)\\n\\tat scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)\\n\\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n\"\n  }\n}\nIt means the synchronous job timed out. It does not mean the job is not running or was stopped. In order to know what happened to the job, one can use either the CLI or the spark-jobserver console.\n\nUsing Spark-Jobserver to get information about jobs\nAs discussed above, it’s possible to connect to the spark-jobserver console in order to get a list of recently run jobs and their results.\n\n\nUsing the CLI to get information about jobs\nFrom the CLI, the approach is different. The main difference is that we first have to retrieve the list of jobs but can not simply click on the last job in the list.\nAlternative, with the use of jq and optionally HTTPie, this can be simplified:\nhttp localhost:8090/jobs | jq 'first'\nThe http command in this example can be replaced by curl. If jq is not installed on your system, a simple |head may suffice to get the job id of the last job.\nBy copy/pasting the job id, we can request the result of one specific job.\nhttp localhost:/8090/jobs/&lt;jobId&gt;\nThe following is a one-liner based on HTTPie and jq:\nhttp localhost:8090/jobs | jq 'first' | jq -r '.jobId' | xargs -I{} http localhost:8090/jobs/{}\n\n\n\n\n\n\n\n\n\n\nA tool in the LuciusOperations toolbox will be created in the near future, allowing to query the jobs easily and retrieve the result for the last job."
  }
]
[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "LuciusOperations",
    "section": "",
    "text": "The glue that holds the Compass stack together\n\n\n\nThis repo is a toolbox for the maintenance of the Lucius stack. The main goal is to make life easier initializing/running/checking the APIs that are part of the stack: LuciusAPI and LuciusProcessing. We might add functionality for Sourire (and perhaps even Brutus) later as well.\nLuciusAPI and LuciusProcessing are two REST APIs that are both endpoints defined using the Spark-Jobserver project and that expose complementary functionality:\n\n\nLuciusProcessing deals with preparing source data into a format that can be initialized for LuciusAPI.\nLuciusProcessing only exposes 2 endpoints: a process endpoint for processing the data from input to LuciusAPI-ready output and a check endpoint for retrieving information about source and (pre-computed) output.\n\n\n\nLuciusAPI is the engine for the LuciusWeb web frontend. It defines endpoints for all the queries and computations that need to be performed in the application. A Spark context is running 24x7 and LuciusAPI acts as the interface to that Spark context.\nLuciusAPI contains many more endpoints for every kind of query of computation that needs to be available to the frontend. The concrete implementation of these endpoints is provided by the [LuciusCore] library: Zhang correlation calculation, histogram, …\n\n\n\n\nConnecting to those APIs is not hard: in principle one sends a configuration object to the API and one gets a JSON object back. It does requires a very strict format for input and parameters and is cumbersome to write though.\nIn order to aid in connecting to the APIs and as such also perform basic processing and initialization tasks, we developed the LuciusOperations toolbox.\n\n\n\nThe short version is to fetch the LuciusOperations repository, make sure you have Java 8 or higher installed and run bin/build.sh. After this, the tools for LuciusProcessing and LuciusAPI are respectively under utils/processing and utils/api:\nutils\n├── api\n│   ├── check\n│   ├── create_context\n│   ├── fetch_jar\n│   ├── initialize\n│   ├── remove_context\n│   └── upload_jar\n└── processing\n├── check\n├── create_context\n├── fetch_jar\n├── process\n├── remove_context\n└── upload_jar\nEvery tool in the toolbox is a (Viash) standalone script that contains it’s own help, i.e.:\n❯ utils/processing/check -h\ncheck dev\n\nArguments:\n--endpoint\ntype: string\ndefault: http://localhost:8090\nThe endpoint (URL) to connect to\n\n...\nPlease refer to the installation instructions for more information about what is happening behind the scenes.\n\n\n\nA typical workflow would look like this:\n\nEnsure the spark-jobserver is running\nMake sure the JAR files for LuciusAPI and LuciusProcessing are available in a local directory, preferably one next to the other. Those JAR files are typically built using a CI pipeline, but can be fetched from the github repository as well.\nUpdate the _viash.yaml configuration file for your environment.\nrun bin/build.sh again, as this will build the different tools with the correct defaults as provided in the _viash.yaml config file.\nRun the following commands:\nutils/processing/upload_jar        # Upload both JAR files\nutils/api/upload_jar               \nutils/processing/create_context    # create the context\nutils/processing/process           # start the processing job\nutils/processing/check             # verify if output is written\nutils/api/initialize               # initialize Spark job\nutils/api/check                    # after some time, check\n\n\n\n\nThe following are required in order to run the components from the toolbox:\n\nwget\nbash\ncurl\n\nOne day, we might allow HTTPie to be used for a more user friendly way of calling the endpoints."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation Instructions",
    "section": "",
    "text": "LuciusOperations uses Viash to convert relatively simple scripts containing REST calls into configurable CLI tools.\nThe use of Viash and other applications of the tool is outside the scope of this document. Suffice to note that Viash uses a concept of source and what is built from that source. The source of the components can be found under src/, using the bin/build.sh wrapper script, the built scripts are under utils/.\n\nbin/build.sh\n\n\n>> Building both namespaces\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/processing/check/config.vsh.yaml' failed\u001b[0m\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/processing/process/config.vsh.yaml' failed\u001b[0m\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/api/initialize/config.vsh.yaml' failed\u001b[0m\nExporting create_context (processing) =native=> utils/processing\nExporting upload_jar (processing) =native=> utils/processing\nExporting remove_context (processing) =native=> utils/processing\nExporting fetch_jar (processing) =native=> utils/processing\n\u001b[33mNot all configs built successfully\u001b[0m\n  \u001b[31m3/12 configs encountered parse errors\u001b[0m\n  \u001b[33m5/12 configs were disabled\u001b[0m\n  \u001b[32m4/12 configs built successfully\u001b[0m\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/processing/check/config.vsh.yaml' failed\u001b[0m\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/processing/process/config.vsh.yaml' failed\u001b[0m\n\u001b[31mReading file '/Users/toni/code/compass/LuciusOperations/src/api/initialize/config.vsh.yaml' failed\u001b[0m\nExporting create_context (api) =native=> utils/api\nExporting upload_jar (api) =native=> utils/api\nExporting remove_context (api) =native=> utils/api\nExporting fetch_jar (api) =native=> utils/api\nExporting check (api) =native=> utils/api\n\u001b[33mNot all configs built successfully\u001b[0m\n  \u001b[31m3/12 configs encountered parse errors\u001b[0m\n  \u001b[33m4/12 configs were disabled\u001b[0m\n  \u001b[32m5/12 configs built successfully\u001b[0m\n\n>> Please see under utils/ to find the tools for api and processing...\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis description deals with the installation and configuration of LuciusOperations only."
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Usage",
    "section": "",
    "text": "If the LuciusWeb interface is able to calculate Zhang scores and top tables sufficiently fast, it is because the effective calculations are performed in a distributed way by Spark running on a cluster. In order for Spark to do it’s parallel magic, though, it needs the data to be in a suitable format. Every lookup or join that would have to be done during execution of a query would be detremental to the overall performance. In other words, for Spark to function effectively we need to prepare the data in a format that would be called denormalized in traditional database terms.\nWhat this means is that the database we work with on the level of the cluster is a (very) long list of complete records. If a specific compound appears 100 times in this database, than we store all that needs to be shown in the tables directly with every record where this compound is the treatment. This approach trades in storage efficiency for processing speed.\nWe distinguish 3 main steps in preparing the data:\n\n\n\n\nflowchart LR\n  A[raw data] -- preprocessing --> B[preprocessed data per batch]\n  B -- LuciusProcessing --> C[ready for Lucius]\n\n\n\n\n\n\n\n\nThe preprocessing step takes in the raw data from the experiments and applies differential analysis, possibly replicate consolidation and other computational steps. We will not discuss the preprocessing step as it is outside the scope of our work. The result of this step is written to one or more data files as CSV, TSV, Parquet or some other structured data format. Typically this preprocessed data is structured by batch of the original raw data. Additional information should be provided with it dealing with annotations for treatments, genes and samples.\nWe pick up the data at this stage and prepare it for use with Lucius. That’s what LuciusProcessing is for."
  },
  {
    "objectID": "installation.html#technical-details",
    "href": "installation.html#technical-details",
    "title": "Installation Instructions",
    "section": "Technical details",
    "text": "Technical details\nEvery Viash component contains a script.sh file (usually some parameter handling and a command to execute) and a config.vsh.yaml file which contains the configuration for the component.\nBy running bin/build.sh, this combination of files is transformed into 1 executable script that performs essentially two things:\n\nThe resulting executable contains the argument parsing necessary to run it from the CLI\nThe defaults configured in _viash.yaml are applied.\n\nAn illustration of the CLI argument parsing capabilities:\n\nutils/processing/check -h | head\n\ncheck dev\n\nArguments:\n    --endpoint\n        type: string\n        default: http://localhost:8090\n        The endpoint (URL) to connect to\n\n    --application\n        type: string\n\n\nThe format of the configuration in _viash.yaml is derived from the way jq allows to query and update JSON blobs. It should be clear from the example in the repository how to use it:\n\nsource: src\ntarget: target\n\nconfig_mods: |\n  .functionality.version := 'dev'\n  .functionality.arguments[.name == '--application'].default := 'luciusprocessing-dev'\n  .functionality.arguments[.name == '--db'].default := '<pointer to processed data location>'\n  .functionality.arguments[.name == '--db_version'].default := '<version>'\n  .functionality.arguments[.name == '--input'].default := '<pointer to original batches>'\n  .functionality.arguments[.name == '--geneAnnotations'].default := '<pointer to gene annotation file>'\n  .functionality.arguments[.name == '--treatmentAnnotations'].default := '<pointer to treatment annotation file>'\n  .functionality.arguments[.name == '--cellAnnotations'].default := '<pointer to cell annotation file>'\n\nThe bin/build.sh script uses Viash to create 2 subdirectories under utils/ containing tools that are used for the processing part and for the api part:\n\ntree utils/\n\nutils/\n├── api\n│   ├── check\n│   ├── create_context\n│   ├── fetch_jar\n│   ├── initialize\n│   ├── remove_context\n│   └── upload_jar\n└── processing\n    ├── check\n    ├── create_context\n    ├── fetch_jar\n    ├── process\n    ├── remove_context\n    └── upload_jar\n\n2 directories, 12 files\n\n\nPlease refer to the usage guide for more information about how to use the tools under utils/."
  },
  {
    "objectID": "usage.html#source-data",
    "href": "usage.html#source-data",
    "title": "Usage",
    "section": "Source data",
    "text": "Source data\nThe source data that the processing step takes in is the result of differential analysis, possibly replicate consolidation and other computational steps. Typically, this"
  },
  {
    "objectID": "usage.html#luciusprocessing",
    "href": "usage.html#luciusprocessing",
    "title": "Usage",
    "section": "LuciusProcessing",
    "text": "LuciusProcessing\n\nSetup\nWe first have to initialize a spark-jobserver (and consequently Spark) context in which our subsequent jobs will run. We assume the bin/build.sh has been run already, that leaves us with:\nutils/processing/create_context\nYou should receive a response containing SUCCESS. If you get a timeout message, please try again.\nWe should provide the appropriate JAR file to the jobserver as well:\nutils/processing/upload_jar \\\n  --jars ... \\           # location of the jar file\n  --tag ...  \\           # version of LuciusAPI to use\n  --...\nPlease check if those arguments are not already set correctly in the _viash.yaml project config file.\n\n\n\n\n\n\nNote\n\n\n\nIf you decide to update the defaults in _viash.yaml, make sure to run bin/build.sh again!\n\n\nAfter you issued the previous command, you should receive a response saying the JAR is uploaded.\nIf you get something like this it means something is wrong with the JAR file:\n{\n  \"status\": \"ERROR\",\n  \"result\": \"Binary is not of the right format\"\n}\n\n\nIntroduction\nLuciusProcessing transforms the data from preprocessed (per batch) data that is normalized to denormalized data in one or multiple ‘files’ per version (see later).\n\n\n\n\n\n\nNote\n\n\n\nIn what follows, we assume the preprocessed data is in Parquet format as well.\n\n\nThe source should look like this:\ninput/<batch>_profile.parquet           # profile data (t-stats, p values, ...)\ninput/<batch>_profile_meta.parquet      # profile meta data (treatment, sample data, ...)\ngeneAnnotations                         # a 'file' containing gene annotations\ntreatmentAnnotations                    # a 'file' containing treatment annotations\ncellAnnotations                         # a 'file' containing cell annotations\nTypically, those files and directories will be on a shared filesystem or blob storage: S3 or HDFS.\nEach <batch> of data might be added on a different date because experimental data is added. The preprocessed data we consume with LuciusProcessing has two modes:\n\nThe default mode, where all data from input/... is fetched and processed. This results in a new major version.\nAn incremental mode, used to process new data since the last processing run.\n\n\n\nFull processing\nLet’s take a look at the workflow to better understand what happens. We initialized a context and uploaded a JAR above, it’s now time to see if our config is right and if the data is available:\nutils/processing/check\nHere, we assume the default arguments are correctly configured in _viash.yaml, if in doubt you can always run utils/processing/check -h.\n\n\n\n\n\n\nRemark\n\n\n\nWe configured the check request to be synchronous. That means that the check tool will wait for the answer to be returned. There is however a (configurable) timeout for synchronous requests, and so it may be the returned status is status: ERROR. Don’t worry in that case, there are ways to get to the requested information. We’ll discuss them later.\n\n\nThis is the output of check on a limited dataset, trimmed and formatted to be better readable:\n{\n  \"duration\": \"12.053 secs\",\n  \"classPath\": \"com.dataintuitive.luciusprocessing.check\",\n  \"startTime\": \"2022-11-30T13:14:01.863+01:00\",\n  \"context\": \"luciusapi\",\n  \"result\": {\n    \"info\": \"No data to process, please check input and parameters\",\n    \"header\": \"None\",\n    \"data\": {\n      \"inputPath\": \".../\",\n      \"inputs\": [\n        \"batch1: 2022-10-25 with 162 samples stored in .../batch1_profile_meta.parquet\",\n        \"batch2: 2022-10-25 with 84 samples stored in .../batch2_profile_meta/ASG001_MCF7_6H_l5_profile_meta.parquet\",\n        ...\n        ],\n      \"filter_inputs\": [\n        ...\n      ],\n      \"outputs\": [\n        \"Version 6_0 at 2022-11-28 (.../output-data/2022-11-28_output_v6_0.parquet)\",\n        \"Version 1_0 at 2022-12-01 (.../output-data/2022-12-01_output_v1_0.parquet)\",\n        ...\n        ]\n    }\n  },\n  \"status\": \"FINISHED\",\n  \"jobId\": \"c6ea3c8a-dcd1-41bb-bf28-69c46e5431f7\",\n  \"contextId\": \"\"\n}\nThere are 3 important lists in the output above:\n\ninputs is a list of the preprocessed batches that are available.\nfilter_inputs is for running incrementally, we’ll discuss that later.\noutputs is a list of already processed files.\n\nIf you add --processingDate to the check tool, only data before that date will be processed. By default it is the current date of processing, but in certain situations it might be useful to be able to set it explicitly. For instance, if you want to process the a large number of batches in pieces based on the date at which they were added. Or just when preparing a test dataset. But in general, it should not be used.\nIf you specify the --processingDate, the filter_inputs list will contain the entries that match the query.\nWhen the output of the check tool yields the expected result, it’s time to start processinsg the data. If processed data is available in the outputs list, the major version will automatically be updated when a full processing run is started. For example, if the latest output dataset is version 3.x, the next full processing run will get version 4.\nThe dates and versions are only encoded in the filenames of the Parquet files for convenience. The dates and version that are effectively used are encoded inside thte files. So even if we rename files or move them, we are able to retrieve that information.\nIt’s now time to start the effective processing job:\nutils/processing/process\nAgain, this just works if the _viash.yaml config file has been properly provisioned or configured.\nThe process tool does not wait for the result and runs in the background. One can either connect to the jobserver console or via the CLI (see later).\n\n\nIncremental processing runs\nThe process for incremental runs is the same as for full runs, the difference is in the selection of data that is used for processing: an incremental run looks at the last major version in the output location, and for that version the last minor version. It then checks at what date that data was processed. If there is input data that is newer than the last processed data, it will be processed and the minor version will be bumped.\nRunning the processing is as easy as before by adding --incremental:\nutils/processing/process --incremental\nRemember that the filtered_inputs will contain the entries in the input that would be processed when running incrementally.\nIn order to cleanup, it’s good practice to remove the context after we ran the processing:\nutils/processing/remove_context"
  },
  {
    "objectID": "usage.html#troubleshooting",
    "href": "usage.html#troubleshooting",
    "title": "Usage",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGetting job output when the request times out\nSometimes, the following message or something similar can be returned:\n{\n  \"status\": \"ERROR\",\n  \"result\": {\n    \"message\": \"Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\",\n    \"errorClass\": \"akka.pattern.AskTimeoutException\",\n    \"stack\": \"akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\\n\\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)\\n\\tat akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)\\n\\tat scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)\\n\\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n\"\n  }\n}\nIt means the synchronous job timed out. It does not mean the job is not running or was stopped. In order to know what happened to the job, one can use either the CLI or the spark-jobserver console.\n\nUsing Spark-Jobserver to get information about jobs\nConnect to the URL of the spark-jobserver instance in your environment. If you don’t know what that is, but you’re LuciusOperations toolbox has been configured correctly, you can retrieve that information from the output of utils/processing/check -h. The first argument in the help message is --endpoint and the default value for it is the URL you need:\n\nutils/processing/check -h | head -8\n\ncheck dev\n\nArguments:\n    --endpoint\n        type: string\n        default: http://localhost:8090\n        The endpoint (URL) to connect to\n\n\nOpen a browser tab and connect to this URL, you should see a screen similar to the one below.\n\n\n\nSpark Jobserver console\n\n\nThere are three tabs here:\n\nJobs: to retrieve a list of recently run jobs split in 3 ‘states’: Running, Completed and Failed.\nContexts: to retrieve a list of contexts that are services from this jobserver instance. This should correspond to the context created using utils/processing/create_context.\nBinaries: this tab lists the JAR files that have been uploaded using utils/processing/upload_jar.\n\nClicking on the the job link in the Jobs tab, you get the JSON result of the job. The (C) link retrieves the configuration that was used for this job.\nIf you know the job number, you could alternatively open the job page directly:\n<endpoint>/jobs/<jobID>\n\n\nUsing the CLI to get information about jobs\nFrom the CLI, the approach is different. The main difference is that we first have to retrieve the list of jobs but can not simply click on the last job in the list.\nAlternative, with the use of jq and optionally HTTPie, this can be simplified:\nhttp localhost:8090/jobs | jq 'first'\nThe http command in this example can be replaced by curl. If jq is not installed on your system, a simple |head may suffice to get the job id of the last job.\nBy copy/pasting the job id, we can request the result of one specific job.\nhttp localhost:/8090/jobs/<jobId>\nThe following is a one-liner based on HTTPie and jq:\nhttp localhost:8090/jobs | jq 'first' | jq -r '.jobId' | xargs -I{} http localhost:8090/jobs/{}\n\n\n\n\n\n\nNote\n\n\n\nA tool in the LuciusOperations toolbox will be created in the near future, allowing to query the jobs easily and retrieve the result for the last job."
  },
  {
    "objectID": "usage.html#luciusapi",
    "href": "usage.html#luciusapi",
    "title": "Usage",
    "section": "LuciusAPI",
    "text": "LuciusAPI\nOnce there data is processed for use with Lucius, it’s a matter of initializing the long-running context. This is similar to what we did above:\nutils/api/create_context\nutils/api/upload_jar\nutils/api/initialize\nAt this point, the data will be loaded and cached in memory on the Spark cluster. The caching is important for performance. You can check the status of the initialization job using the jobserver console or the CLI (see below under Troubleshooting).\nPlease note that while the processing step only requires the profiles, profile meta and gene annotations, LuciusAPI also needs treatment annotations and cell annotations. Those have to be provided.\nWhen the initialization job has finished, a check can be performed:\nutils/api/check\nYou’ll get some basic statistics about what is available in the dataset. If this works, there’s a high chance that the frontend will work!\n\n\n\n\n\n\nWarning\n\n\n\nDo not remove the context unless you really intend to do that: removing the context will stop the application from working.\n\n\n\nTechnical details\nEvery tool in the toolbox in fact performs a POST REST request to LuciusAPI or LuciusProcessing. This POST request includes a so-called payload or a configuration object for this specific endpoint or function. The processing/process and api/initialize also require such a configuration payload. The template for this can be found under etc/. There is one config file for LuciusAPI and one for LuciusProcessing. A simple templating mechanism is used where variables that require subsitition are enclosed by two underscores (__)."
  },
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Connect to the URL of the spark-jobserver instance in your environment. If you don’t know what that is, but you’re LuciusOperations toolbox has been configured correctly, you can retrieve that information from the output of utils/processing/check -h. The first argument in the help message is --endpoint and the default value for it is the URL you need:\nutils/processing/check -h | head -8\nOpen a browser tab and connect to this URL, you should see a screen similar to the one below.\n\n\n\nSpark Jobserver console\n\n\nThere are three tabs here:\n\nJobs: to retrieve a list of recently run jobs split in 3 ‘states’: Running, Completed and Failed.\nContexts: to retrieve a list of contexts that are services from this jobserver instance. This should correspond to the context created using utils/processing/create_context.\nBinaries: this tab lists the JAR files that have been uploaded using utils/processing/upload_jar.\n\nClicking on the the job link in the Jobs tab, you get the JSON result of the job. The (C) link retrieves the configuration that was used for this job.\nIf you know the job number, you could alternatively open the job page directly:\n<endpoint>/jobs/<jobID>"
  },
  {
    "objectID": "troubleshooting.html#getting-job-output-when-the-request-times-out",
    "href": "troubleshooting.html#getting-job-output-when-the-request-times-out",
    "title": "Troubleshooting",
    "section": "Getting job output when the request times out",
    "text": "Getting job output when the request times out\nSometimes, the following message or something similar can be returned:\n{\n  \"status\": \"ERROR\",\n  \"result\": {\n    \"message\": \"Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\",\n    \"errorClass\": \"akka.pattern.AskTimeoutException\",\n    \"stack\": \"akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://JobServer/user/context-supervisor/luciusapi#-1556520586]] after [10000 ms]. Sender[null] sent message of type \\\"spark.jobserver.JobManagerActor$StartJob\\\".\\n\\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)\\n\\tat akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)\\n\\tat scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)\\n\\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)\\n\\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)\\n\\tat akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n\"\n  }\n}\nIt means the synchronous job timed out. It does not mean the job is not running or was stopped. In order to know what happened to the job, one can use either the CLI or the spark-jobserver console.\n\nUsing Spark-Jobserver to get information about jobs\nAs discussed above, it’s possible to connect to the spark-jobserver console in order to get a list of recently run jobs and their results.\n\n\nUsing the CLI to get information about jobs\nFrom the CLI, the approach is different. The main difference is that we first have to retrieve the list of jobs but can not simply click on the last job in the list.\nAlternative, with the use of jq and optionally HTTPie, this can be simplified:\nhttp localhost:8090/jobs | jq 'first'\nThe http command in this example can be replaced by curl. If jq is not installed on your system, a simple |head may suffice to get the job id of the last job.\nBy copy/pasting the job id, we can request the result of one specific job.\nhttp localhost:/8090/jobs/<jobId>\nThe following is a one-liner based on HTTPie and jq:\nhttp localhost:8090/jobs | jq 'first' | jq -r '.jobId' | xargs -I{} http localhost:8090/jobs/{}\n\n\n\n\n\n\nNote\n\n\n\nA tool in the LuciusOperations toolbox will be created in the near future, allowing to query the jobs easily and retrieve the result for the last job."
  },
  {
    "objectID": "usage.html#customizations",
    "href": "usage.html#customizations",
    "title": "Usage",
    "section": "Customizations",
    "text": "Customizations\n\nProcessed data versions\nAs noted above, processed data is stored with a major/minor versioning scheme: major versions for full processing runs, minor versions for incremental runs. Each new full run generates a new major version and consequently the utils/api/initialization tool should pick out the correct one to initialize the context.\nThe default is set to 0, but one will often require different versions. One approach is to update the _viash.yaml file and encode the proper default there.\nAnother approach is to simply uses the arguments to modify the behavior of the initialize tool:\nutils/api/initialize --db_version 2\nPlease note that we only select a major version because all minor versions are incremental runs on top of version 2.0.\n\n\nOther customizations\nIt’s possible to edit the _viash.yaml file and run bin/build.sh again in order to create the utils/ toolbox with new defaults. It’s also possible to use the arguments available, just be sure to use them consistenly.\nFor instance:\nbin/processing/create_context \\\n  --application my_app_name       # specify a different name\nbin/processing/upload_jar \\\n  --tag 0.2.0                     # select an older version\nbin/processing/check \\\n  --application my_app_name       # use the same app name as before\nbin/api/initialize \\\n  --db $different_path \\          # point to different database location\n  --db_version 1  \\               # point to different database version\n  --application my_app_name\nAs always, simply call the appropriate tool from the toolbox with -h or --help.\nIn principle, we could create scripts (or even workflows?) based on these steps."
  },
  {
    "objectID": "usage.html#processed-data-versions",
    "href": "usage.html#processed-data-versions",
    "title": "Usage",
    "section": "Processed data versions",
    "text": "Processed data versions\nAs noted above, processed data is stored with a major/minor versioning scheme: major versions for full processing runs, minor versions for incremental runs. Each new full run generates a new major version and consequently the utils/api/initialization tool should pick out the correct one to initialize the context.\nThe default is set to 0, but one will often require different versions. One approach is to update the _viash.yaml file and encode the proper default there.\nAnother approach is to simply uses the arguments to modify the behavior of the initialize tool:\nutils/api/initialize --db_version 2\nPlease note that we only select a major version because all minor versions are incremental runs on top of version 2.0.\n\nOther customizations\nIt’s possible to edit the _viash.yaml file and run bin/build.sh again in order to create the utils/ toolbox with new defaults. It’s also possible to use the arguments available, just be sure to use them consistenly.\nFor instance:\nbin/processing/create_context \\\n  --application my_app_name       # specify a different name\nbin/processing/upload_jar \\\n  --tag 0.2.0                     # select an older version\nbin/processing/check \\\n  --application my_app_name       # use the same app name as before\nbin/api/initialize \\\n  --db $different_path \\          # point to different database location\n  --db_version 1  \\               # point to different database version\n  --application my_app_name\nAs always, simply call the appropriate tool from the toolbox with -h or --help.\nIn principle, we could create scripts (or even workflows?) based on these steps."
  }
]